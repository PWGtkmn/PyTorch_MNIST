{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST(kaggleバージョン)をPytorchで実行する。\n",
    "### やること\n",
    "1. データの読み込み〜Dataset化\n",
    "2. Dataloaderの定義\n",
    "3. ネットワークの定義\n",
    "4. 学習処理の定義\n",
    "5. 検証用データを用いたモデルの検証\n",
    "6. テストデータへの適用\n",
    "\n",
    "### 参考\n",
    "- Dataset, DataLoader：  \n",
    "    - https://www.kaggle.com/pinocookie/pytorch-dataset-and-dataloader  \n",
    "    - https://qiita.com/mathlive/items/2a512831878b8018db02\n",
    "- 学習処理の記述方法：\n",
    "    - https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py\n",
    "- データランダムな分割の方法：\n",
    "    - https://qiita.com/takurooo/items/ba8c509eaab080e2752c\n",
    "- Epochごとのモデルの検証：\n",
    "    - https://axa.biopapyrus.jp/deep-learning/pytorch/image-classification.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__() ## run the Parent Class's .__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3, padding=1) # (in_channnel, out_channel, kernel_size)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2_drop = nn.Dropout2d(p=0.6)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(784, 240)  # paddingすることで画像サイズは据え置きになるため784とする。\n",
    "        self.fc2 = nn.Linear(240, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.fc4 = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), kernel_size=(2, 2)) ## J\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2_drop(self.conv2(x))), (2, 2))\n",
    "        x = x.view(-1, self.num_flat_features(x)) # x.shape = (8,400)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net().double()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformの定義(このNotebookでは使用しない。勉強のための定義のみ。)\n",
    "- ここでは、データを格納しているTensorを元にDatasetクラスを作成し、Pytorch上で扱いやすくする。\n",
    "- TorchVision標準のデータセットを用いる場合、以下の流れとなるらしい。(今回は不使用。)\n",
    "    - transformsによる前処理の定義(torchvision)\n",
    "        - transform：データの前処理全般を定義するもの。\n",
    "        - 今回は各pixelの値を255.0で除算する前処理を入れる。\n",
    "        - 画像なら(縦、横、チャネル)->(チャネル、縦、横)と変換する、ピクセル数を補正する、左右反転、回転させるなどの機能がある。\n",
    "    - Datasetsによる前処理&ダウンロード\n",
    "    - DataloaderによるDatasetの使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransform():\n",
    "    \"\"\"\n",
    "    画像に対する前処理を行うクラス。以下の機能を持つ。\n",
    "    pandas.DataFrameを入力としてTensorに変換する。\n",
    "    その後、各pixelの値を0〜1にするため、255で除算する。\n",
    "    なお、学習時と診断時での処理内容は同じであるとする。\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, df_pic):\n",
    "        \"\"\"\n",
    "        学習時と診断時での前処理は同じものとする。\n",
    "        本当は学習時のみノイズを加えたり色々する。\n",
    "        \"\"\"\n",
    "        out_label = df_pic.label\n",
    "        out_img = df_pic.iloc[:,1:].values.astype(np.uint8).reshape((1, 28, 28))\n",
    "        out_label = torch.tensor(y_train.values)\n",
    "        out_img = torch.tensor(x_train.values)\n",
    "        out_img = out_img/255.0\n",
    "        return out_img, out_label\n",
    "\n",
    "trans = ImageTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSetsの作成\n",
    "- torch.utils.data.Datasetクラスを継承して作成する。\n",
    "- メソッドには以下がある。\n",
    "    - init:コンストラクタ、クラス定義時に実行され、値の初期化などを行う。\n",
    "    - len:画像の枚数を返す。\n",
    "    - getitem:Datasetからデータを1レコード分返す。DataLoaderに呼び出されている？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDatasets(data.Dataset):\n",
    "    def __init__(self, input_filename, transform = None):\n",
    "        self.data = pd.read_csv(input_filename)\n",
    "        self.transform = trans\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.data.shape[1]==784:\n",
    "            img_transformed = self.data.iloc[idx,:].values.astype(np.uint8).reshape((1, 28, 28))/255.0\n",
    "            return img_transformed\n",
    "        else:\n",
    "            img_transformed = self.data.iloc[idx, 1:].values.astype(np.uint8).reshape((1, 28, 28))/255.0\n",
    "            img_label = self.data.iloc[idx, 0]\n",
    "            return img_transformed, img_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 教師データの学習用データと検証用データへの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_supervised = MnistDatasets(\"./01_input/train.csv\")\n",
    "n_eval = 2000\n",
    "data_train, data_eval = data.random_split(data_supervised, [42000-n_eval, n_eval])\n",
    "data_test = MnistDatasets(\"./01_input/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaderを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_minibatch = 8\n",
    "trainloader = data.DataLoader(data_train, batch_size=n_minibatch, shuffle=False)\n",
    "evalloader = data.DataLoader(data_eval, batch_size=n_minibatch, shuffle=False)\n",
    "testloader = data.DataLoader(data_test, batch_size=n_minibatch, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失関数と最適化手法を定義する\n",
    "損失関数には交差エントロピー損失を使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), weight_decay=0.0005)\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.0005, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## これまでに作成した関数を用いて学習を行なう。\n",
    "- パラメータの初期化\n",
    "- for epoch\n",
    "    - for ミニバッチ学習\n",
    "        - 順伝播\n",
    "        - 損失の計算\n",
    "        - パラメータ最適化\n",
    "            - 損失の勾配の計算\n",
    "            - パラメータの更新\n",
    "    - for 検証用データでテスト\n",
    "        - 順伝播\n",
    "        - 損失の計算\n",
    "- todo：学習/検証の処理フェーズを条件分岐で記述する。⇒TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "    for i, data_jj in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data_jj\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs.double())\n",
    "        train_loss = criterion(outputs, labels)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += train_loss.item()\n",
    "        if i % 5000 == 4999:    # print every 5000 mini-batches\n",
    "            print('[%d, %5d] Training Loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 5000))\n",
    "            running_loss = 0.0\n",
    "    print(\"Epoch No.\" + str(epoch+1) + \" has finished!\")\n",
    "    \n",
    "    eval_loss = 0\n",
    "    net.eval()\n",
    "    for i, data_jj in enumerate(evalloader, 0):\n",
    "        inputs, labels = data_jj\n",
    "        outputs = net(inputs.double())\n",
    "        eval_loss = criterion(outputs, labels)\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += eval_loss.item()\n",
    "    \n",
    "    print('Validation Loss: %.3f' %\n",
    "          (running_loss/(n_eval/n_minibatch))) ## eval_lossを追加\n",
    "    print('*****')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './02_output/MNISTmodel.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習済みモデルを用いた予測の実行\n",
    "- メモリにデータが乗り切るよう、100件ごとに予測値を算出する。\n",
    "- PCのメモリが足りず全データを一度に処理できなかったため。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i, data_jj in enumerate(testloader, 0):\n",
    "    inputs = data_jj\n",
    "    tmp_pred = net(inputs.double())\n",
    "    tmp_pred = np.argmax(tmp_pred.detach().numpy().copy(), axis=1)\n",
    "    if a==0:\n",
    "        pred = tmp_pred\n",
    "        a+=1\n",
    "    else:\n",
    "        pred = np.r_[pred, tmp_pred]\n",
    "\n",
    "output = pd.DataFrame(pred)\n",
    "output.columns = [\"Label\"]\n",
    "output.index.name = \"ImageID\"\n",
    "output.index = output.index + 1\n",
    "\n",
    "output.to_csv(\"./02_output/pred_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## その他：Pythonにおけるクラスの概念\n",
    "- \"class hogehoge():\" として定義する。引数には継承元の親クラスが入る。\n",
    "- アンダースコア2本で囲まれるメソッドは特殊メソッドと呼ばれる。\n",
    "    - init：インスタンスを生成する時点で実行される。\n",
    "    - call：生成したインスタンスを()つきで実行すると実行される。\n",
    "    - getitem:生成したインスタンスを[]つきで実行すると実行される。鍵カッコにはインデックスを示す整数が入る。\n",
    "- self.super().｛メソッド名｝とすると親クラスのメソッドを利用できる。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
